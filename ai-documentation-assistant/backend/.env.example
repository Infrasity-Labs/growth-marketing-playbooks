# =============================================================================
# AI Documentation Assistant - Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your values

# Product Configuration
# ---------------------
# The name of your product/documentation (used in AI responses, UI, and health checks)
# This value is used throughout the application to personalize responses
# Examples: "My API Docs", "Product Documentation", "Knowledge Base", "Kubiya Docs"
# Important: Update this in chat-widget.js USER_CONFIG.PRODUCT_NAME as well if not using API sync
PRODUCT_NAME=My Documentation

# Ollama Configuration
# ---------------------
# Ollama server URL (default: http://localhost:11434)
OLLAMA_URL=http://localhost:11434

# LLM model for text generation
# Options: llama3.2:1b, llama3.2:3b, llama3.1:8b, mistral, etc.
LLM_MODEL=llama3.2:1b

# Embedding model for vector search
EMBEDDING_MODEL=all-minilm

# Flask Configuration
FLASK_DEBUG=false
PORT=5000
HOST=0.0.0.0

# CORS Settings (comma-separated origins)
# Add your frontend URL here to enable cross-origin requests
CORS_ORIGINS=http://localhost:3000,http://127.0.0.1:3000

# RAG Configuration
# ---------------------
# DOCS_PATH: Relative path to documentation files from backend directory
# CHROMA_DB_PATH: Where to store the vector database
# CHUNK_SIZE: Size of document chunks for embedding
# CHUNK_OVERLAP: Overlap between chunks for better context
# RETRIEVAL_K: Number of relevant documents to retrieve per query
DOCS_PATH=../
CHROMA_DB_PATH=./chroma_db
CHUNK_SIZE=500
CHUNK_OVERLAP=50
RETRIEVAL_K=2

# LLM Settings
# Temperature controls randomness (0 = deterministic, 1 = random)
LLM_TEMPERATURE=0.7
